{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, save_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import keras\n",
    "# from keras.applications.xception import Xception\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg19 import VGG19\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.applications.mobilenetv2 import MobileNetV2\n",
    "# from keras.applications.densenet import DenseNet121\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def display_image(image, title=''):\n",
    "    cv2.imshow(title, image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vgg can't take input of 28x28 so we shall resize all our images\n",
    "pickle_folder = \"pickle files\"\n",
    "input_shape=(84, 84, 1)\n",
    "data = pickle.load(open('data.pickle', 'rb'))\n",
    "# print(\"Data of format:\")\n",
    "# for ch in data:\n",
    "#     print(\"data_length(\" + ch + \") =\", len(data[ch]))\n",
    "\n",
    "#no. of classes\n",
    "classes = len(data.keys())\n",
    "# print(\"\\n\\nClasses:\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groups of most misclassified characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups =    [\n",
    "    \n",
    "    ['उ', 'ऊ', ],\n",
    "    ['अ', 'ऊ', ],\n",
    "    ['स', 'ए', 'र', ],\n",
    "    ['क', 'झ',],\n",
    "    ['श', 'ग', ],\n",
    "    ['घ', 'ध', ],\n",
    "    ['ङ', 'ड',],\n",
    "    ['ज', 'ञ', 'ज्ञ',],\n",
    "    ['ठ', 'ट', 'ढ', ],\n",
    "    ['ढ', 'द',],\n",
    "    ['भ', 'म', 'क्ष'],\n",
    "    ['थ', 'य', ],\n",
    "    ['ब', 'व'],\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: ['/job:localhost/replica:0/task:0/device:GPU:0']\n",
      "input shape: (84, 84, 1)\n",
      "classes: 41\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    " \n",
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "print(\"GPUs:\", k.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "print(\"input shape:\", input_shape)\n",
    "print(\"classes:\", classes)\n",
    "\n",
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1e+03 ns, total: 8 µs\n",
      "Wall time: 13.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = classes\n",
    "epochs = 50\n",
    "k_r = 0.01\n",
    "b_r = 0.01\n",
    "dropout = 0.5\n",
    "\n",
    "def train_CNN(X_train, y_train):\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    model = Sequential()\n",
    "    #conv layer \n",
    "    model.add(Conv2D(32, kernel_size=(9, 9),\n",
    "                     activation='relu',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "                      bias_regularizer=keras.regularizers.l2(b_r),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((3, 3)))\n",
    "    # model.add(Dropout(0.05))\n",
    "    # model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "    #                  activation='relu',\n",
    "    #                   #kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(0.1),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(0.05)))\n",
    "    # model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "    #                  activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r)))\n",
    "#     model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "#                      activation='relu',\n",
    "#                       kernel_initializer='he_normal',\n",
    "#                       kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "#                       bias_regularizer=keras.regularizers.l2(b_r)))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Dropout(dropout))\n",
    "\n",
    "    # model.add(Conv2D(128, kernel_size=(3, 3),\n",
    "    #                  activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r)))\n",
    "    # model.add(Conv2D(128, kernel_size=(3, 3),\n",
    "    #                  activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r)))\n",
    "    # model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Dropout(dropout))\n",
    "\n",
    "    # NN\n",
    "    model.add(Flatten())\n",
    "    #hidden layer 1\n",
    "    # model.add(Dense(1024, activation='relu'))\n",
    "    # model.add(Dropout(0.05))\n",
    "    #hidden layer 2\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(k_r)))\n",
    "    model.add(Dropout(dropout))\n",
    "    #output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    print(\"\\n############################\")\n",
    "    print(\"## Memory usage:\", get_model_memory_usage(batch_size=batch_size, model=model), \"GB ##\")\n",
    "    print(\"############################\\n\")\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_split=0.15,\n",
    "            callbacks = [early_stopping_callback])\n",
    "    return model\n",
    "# score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing models for different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickled\n"
     ]
    }
   ],
   "source": [
    "file = \"class_numerals_{}.pickle\".format(classes)\n",
    "if file in os.listdir(pickle_folder):\n",
    "    class_numerals = pickle.load(open(os.path.join(pickle_folder, file), 'rb'))\n",
    "    print(\"Unpickled\")\n",
    "else:\n",
    "    class_numerals = {i:list(data.keys())[i] for i in range(classes)}\n",
    "    pickle.dump(class_numerals, open(os.path.join(pickle_folder, file), 'wb'))\n",
    "    print(\"Error. Incompatible. Pickled new file:\", file)\n",
    "def character_to_encoding(char, classes=classes):\n",
    "    for i in class_numerals:\n",
    "        if class_numerals[i] == char:\n",
    "            return np_utils.to_categorical(i, classes)\n",
    "\n",
    "def encoding_to_character(encoding):\n",
    "    return class_numerals[encoding.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "write_folder = \"pickle files/group_CNNs/\"\n",
    "\n",
    "for group in groups:\n",
    "    #for each group\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    #creating our dataset\n",
    "    for ch in group:\n",
    "        X += data[ch]\n",
    "        y += [ch] * len(data[ch])\n",
    "        \n",
    "    #resizing input image and preprocessin\n",
    "    X = np.array(list(map(lambda x: cv2.resize(x/255, (input_shape[0], input_shape[1])).reshape((input_shape[0], input_shape[1], 1)), X)))\n",
    "    y = np.array(list(map(lambda x: character_to_encoding(x, classes), y)))\n",
    "\n",
    "    # Train-Test split\n",
    "\n",
    "    train_ratio = 0.8\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=20, shuffle=True, stratify=y)\n",
    "\n",
    "    print(group)\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "#     print(\"\\n\\n\")\n",
    "    \n",
    "    model = train_CNN(X_train, y_train)\n",
    "#     accuracy = model.evaluate(X_test, y_test)\n",
    "    y_test_classed = list(map(encoding_to_character, y_test))\n",
    "    predictions = list(map(encoding_to_character, model.predict(X_test)))\n",
    "    accuracy = accuracy_score(y_pred=predictions, y_true=y_test_classed)\n",
    "    print(\"Accuracy =\", accuracy)\n",
    "    print(confusion_matrix(y_pred=predictions, y_true=y_test_classed))\n",
    "    file = write_folder+'-'.join(group)+'_'+str(accuracy)[:6]+'.h5'\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "#     keras.models.save_model(model, file)\n",
    "#     print(\"Wrote model at\", file)\n",
    "#     print(\"\\n\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
