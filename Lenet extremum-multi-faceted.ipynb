{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "import pandas as pd\n",
    "from IPython.display import display as display_dataframe, clear_output\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, save_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import keras\n",
    "# from keras.applications.xception import Xception\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg19 import VGG19\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.applications.mobilenetv2 import MobileNetV2\n",
    "# from keras.applications.densenet import DenseNet121\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from pympler.tracker import SummaryTracker\n",
    "from shutil import copyfile\n",
    "# from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def display_image(image, title=''):\n",
    "    cv2.imshow(title, image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of format:\n",
      "data_length(छ) = 2000\n",
      "data_length(ह) = 2000\n",
      "data_length(भ) = 2000\n",
      "data_length(घ) = 2000\n",
      "data_length(ब) = 2000\n",
      "data_length(व) = 2000\n",
      "data_length(क) = 2000\n",
      "data_length(ट) = 2000\n",
      "data_length(ज) = 2000\n",
      "data_length(ग) = 2000\n",
      "data_length(ज्ञ) = 2000\n",
      "data_length(ढ) = 2000\n",
      "data_length(ठ) = 2000\n",
      "data_length(ख) = 2000\n",
      "data_length(क्ष) = 2000\n",
      "data_length(य) = 2000\n",
      "data_length(र) = 2000\n",
      "data_length(झ) = 2000\n",
      "data_length(प) = 2000\n",
      "data_length(ष) = 2000\n",
      "data_length(ञ) = 2000\n",
      "data_length(ड) = 2000\n",
      "data_length(त) = 2000\n",
      "data_length(न) = 2000\n",
      "data_length(ङ) = 2000\n",
      "data_length(म) = 2000\n",
      "data_length(द) = 2000\n",
      "data_length(स) = 2000\n",
      "data_length(फ) = 2000\n",
      "data_length(च) = 2000\n",
      "data_length(ण) = 2000\n",
      "data_length(ध) = 2000\n",
      "data_length(श) = 2000\n",
      "data_length(थ) = 2000\n",
      "data_length(त्र) = 2000\n",
      "data_length(ल) = 2000\n"
     ]
    }
   ],
   "source": [
    "#vgg can't take input of 28x28 so we shall resize all our images\n",
    "pickle_folder = \"pickle files\"\n",
    "data = pickle.load(open('data.pickle', 'rb'))\n",
    "print(\"Data of format:\")\n",
    "for ch in data:\n",
    "    print(\"data_length(\" + ch + \") =\", len(data[ch]))\n",
    "\n",
    "#no. of classes\n",
    "classes = len(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rearranging data in input-output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpickled\n"
     ]
    }
   ],
   "source": [
    "file = \"class_numerals_{}.pickle\".format(classes)\n",
    "if file in os.listdir(pickle_folder):\n",
    "    class_numerals = pickle.load(open(os.path.join(pickle_folder, file), 'rb'))\n",
    "    print(\"Unpickled\")\n",
    "else:\n",
    "    class_numerals = {i:list(data.keys())[i] for i in range(classes)}\n",
    "    pickle.dump(class_numerals, open(os.path.join(pickle_folder, file), 'wb'))\n",
    "    print(\"Error. Incompatible. Pickled new file:\", file)\n",
    "def character_to_encoding(char, classes=classes):\n",
    "    for i in class_numerals:\n",
    "        if class_numerals[i] == char:\n",
    "            return np_utils.to_categorical(i, classes)\n",
    "\n",
    "def encoding_to_character(encoding):\n",
    "    return class_numerals[encoding.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: ['/job:localhost/replica:0/task:0/device:GPU:0']\n",
      "classes: 36\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    " \n",
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "print(\"GPUs:\", k.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "# print(\"input shape:\", input_shape)\n",
    "print(\"classes:\", classes)\n",
    "\n",
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = classes\n",
    "epochs = 500\n",
    "k_r = 0.01\n",
    "b_r = 0.01\n",
    "dropout = 0.1\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "input_shape=(32, 32, 1)\n",
    "results = pd.DataFrame(columns=['Char', 'prob'])\n",
    "# # display_dataframe(results)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# tracker = SummaryTracker()\n",
    "\n",
    "for characters in combinations(list(data.keys()), 2):\n",
    "    #selecting labels\n",
    "#     labels = list(data.keys())\n",
    "#     np.random.shuffle(labels)\n",
    "#     labels = labels[:number_of_characters_at_once]\n",
    "#     characters = ['घ', 'ध']\n",
    "#     print(\"\\n\", iteration, labels)\n",
    "\n",
    "    #preparing relevant data\n",
    "    X = []\n",
    "    y = []\n",
    "    for ch in characters:\n",
    "        X += data[ch]\n",
    "        y += [ch] * len(data[ch])\n",
    "    print(characters)\n",
    "\n",
    "    #resizing input image and preprocessin\n",
    "    X = np.array(list(map(lambda x: cv2.resize(x/255, (input_shape[0], input_shape[1])).reshape((input_shape[0], input_shape[1], 1)), X)))\n",
    "    y_ = np.array(list(map(lambda x: character_to_encoding(x, classes), y)))\n",
    "#     print(X.shape, y_.shape)\n",
    "\n",
    "    #train test split\n",
    "    train_ratio = 0.8\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_, train_size=0.8, random_state=20, shuffle=True, stratify=y)\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "    #the model itself\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(6, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(12, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r)\n",
    "                    ))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(120, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r)\n",
    "                    ))\n",
    "\n",
    "    # NN\n",
    "    model.add(Flatten())\n",
    "    #hidden layer 1\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=keras.regularizers.l2(k_r)))\n",
    "    model.add(Dropout(dropout))\n",
    "    #output laye1\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    print(\"\\n############################\")\n",
    "    print(\"## Memory usage:\", get_model_memory_usage(batch_size=batch_size, model=model), \"GB ##\")\n",
    "    print(\"############################\\n\")\n",
    "\n",
    "    #adding custom weights\n",
    "    pre_weights = keras.models.load_model(os.path.join(pickle_folder, \"lenet_relu_3_conv_layer.h5\")).get_weights()\n",
    "    model.set_weights(pre_weights)\n",
    "\n",
    "    #run model\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_split=0.15,\n",
    "            callbacks = [early_stopping_callback])\n",
    "\n",
    "    #saving results\n",
    "    a = model.predict(X_test)\n",
    "    print(characters)\n",
    "    char_indices_in_encodings = [character_to_encoding(characters[i]).argmax() for i in range(2)]\n",
    "    results = pd.DataFrame(a[:, char_indices_in_encodings], columns = characters)\n",
    "    dummy_y_test = np.array(list(map(encoding_to_character, y_test)))\n",
    "    output = np.array(dummy_y_test)\n",
    "    for ch in characters:\n",
    "        output[results[ch] > 0.5] = ch\n",
    "    results['true'] = np.array(list(map(str, output == dummy_y_test)))\n",
    "#     display_dataframe(results)\n",
    "    results.to_excel('pickle files/binary_classification_excel_files/' + '_'.join(characters) +\n",
    "                     '_' + str(model.evaluate(X_test, y_test)[1]) + '.xlsx')\n",
    "    \n",
    "    del X, y, y_, X_train, y_train, X_test, y_test, results, dummy_y_test, a, output, model, pre_weights\n",
    "    k.clear_session()\n",
    "#     tracker.print_diff()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'results/part classifiers/2/'\n",
    "file_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file in ['better_ones', 'worse_ones']:\n",
    "        print(\"skipping folder\", file)\n",
    "        continue\n",
    "    file_list.append(file)\n",
    "    accuracy_list.append(float(file[:-5].split('_')[2]))\n",
    "    \n",
    "from shutil import copyfile\n",
    "#dataframe\n",
    "file_to_accuracy_frame = pd.DataFrame({'file': file_list, 'accuracy': accuracy_list}).sort_values('accuracy')\n",
    "# display_dataframe(file_to_accuracy_frame.head(10))\n",
    "for file in file_to_accuracy_frame['file'].tail(10):\n",
    "    src = folder + file\n",
    "    dst = folder + 'better_ones/' + file\n",
    "    copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for 3 to 35 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 13.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train_lenet(characters):\n",
    "    batch_size = 128\n",
    "    num_classes = classes\n",
    "    epochs = 500\n",
    "    k_r = 0.01\n",
    "    b_r = 0.01\n",
    "    dropout = 0.1\n",
    "\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    input_shape=(32, 32, 1)\n",
    "    results = pd.DataFrame(columns=['Char', 'prob'])\n",
    "    # # display_dataframe(results)\n",
    "\n",
    "#     count = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    for ch in characters:\n",
    "        X += data[ch]\n",
    "        y += [ch] * len(data[ch])\n",
    "    print(characters)\n",
    "\n",
    "    #resizing input image and preprocessin\n",
    "    X = np.array(list(map(lambda x: cv2.resize(x/255, (input_shape[0], input_shape[1])).reshape((input_shape[0], input_shape[1], 1)), X)))\n",
    "    y_ = np.array(list(map(lambda x: character_to_encoding(x, classes), y)))\n",
    "#     print(X.shape, y_.shape)\n",
    "\n",
    "    #train test split\n",
    "    train_ratio = 0.8\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_, train_size=0.8, random_state=20, shuffle=True, stratify=y)\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "    #the model itself\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(6, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(12, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r)\n",
    "                    ))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(120, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "    #                   kernel_initializer='he_normal',\n",
    "    #                   kernel_regularizer=keras.regularizers.l2(k_r),\n",
    "    #                   bias_regularizer=keras.regularizers.l2(b_r)\n",
    "                    ))\n",
    "\n",
    "    # NN\n",
    "    model.add(Flatten())\n",
    "    #hidden layer 1\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=keras.regularizers.l2(k_r)))\n",
    "    model.add(Dropout(dropout))\n",
    "    #output laye1\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    print(\"\\n############################\")\n",
    "    print(\"## Memory usage:\", get_model_memory_usage(batch_size=batch_size, model=model), \"GB ##\")\n",
    "    print(\"############################\\n\")\n",
    "\n",
    "    #adding custom weights\n",
    "    pre_weights = keras.models.load_model(os.path.join(pickle_folder, \"lenet_relu_3_conv_layer.h5\")).get_weights()\n",
    "    model.set_weights(pre_weights)\n",
    "\n",
    "    #run model\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_split=0.15,\n",
    "            callbacks = [early_stopping_callback])\n",
    "    \n",
    "    #saving results\n",
    "    a = model.predict(X_test)\n",
    "#     print(characters)\n",
    "    char_indices_in_encodings = [character_to_encoding(characters[i]).argmax() for i in range(len(characters))]\n",
    "    results = pd.DataFrame(a[:, char_indices_in_encodings], columns = characters)\n",
    "    dummy_y_test = np.array(list(map(encoding_to_character, y_test)))\n",
    "    output = np.array(dummy_y_test)\n",
    "    for ch in characters:\n",
    "        output[results[ch] > 0.5] = ch\n",
    "    results['true'] = np.array(list(map(str, output == dummy_y_test)))\n",
    "#     display_dataframe(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping folder better_ones\n",
      "skipping folder worse_ones\n",
      "[['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'ट', 'स', 'य'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'क्ष', 'ङ', 'ख', 'ग', 'छ', 'म', 'ह', 'ट'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'य', 'ह', 'ज'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'ट', 'य', 'ह'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'क्ष', 'ङ', 'ख', 'ग', 'छ', 'ठ', 'म', 'स'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'य', 'ह', 'ज्ञ'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'य', 'ह', 'म'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'य', 'ट', 'ह'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'य', 'ह', 'ट'], ['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'य', 'ट', 'स']]\n",
      "[['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'क्ष', 'ख', 'त', 'क'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'क्ष', 'क', 'श', 'द'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'श', 'स', 'न', 'त'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'ज', 'झ', 'त', 'ढ'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'क्ष', 'ख', 'त', 'द'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'क्ष', 'क', 'श', 'र'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'स', 'ञ', 'ज', 'श'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'ज', 'झ', 'त', 'फ'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'क्ष', 'ख', 'क', 'ज्ञ'], ['ध', 'घ', 'थ', 'य', 'भ', 'प', 'म', 'च', 'ब', 'व', 'ष', 'ह', 'त्र', 'छ', 'ङ', 'ड', 'श', 'स', 'न', 'द']]\n",
      "['ञ', 'ढ', 'ष', 'झ', 'ल', 'च', 'ण', 'ध', 'र', 'श', 'फ', 'त', 'ख', 'ङ', 'ठ', 'क्ष', 'ग', 'ट', 'स', 'य', 'छ']\n",
      "(33600, 32, 32, 1) (33600, 36) (8400, 32, 32, 1) (8400, 36)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 12)        1812      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 120)         36120     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               61952     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 36)                18468     \n",
      "=================================================================\n",
      "Total params: 118,508\n",
      "Trainable params: 118,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "############################\n",
      "## Memory usage: 0.061 GB ##\n",
      "############################\n",
      "\n",
      "Train on 28560 samples, validate on 5040 samples\n",
      "Epoch 1/500\n",
      "28560/28560 [==============================] - 4s 156us/step - loss: 0.1620 - acc: 0.9724 - val_loss: 0.1001 - val_acc: 0.9907\n",
      "Epoch 2/500\n",
      " 1536/28560 [>.............................] - ETA: 3s - loss: 0.1220 - acc: 0.9798"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-70081179b087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malready_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     results.to_excel(current_folder + '_'.join(characters+[ch]) +\n\u001b[1;32m     36\u001b[0m                                      '_' + str(sum(results['true'] == 'True')/results.shape[0]) + '.xlsx')\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mtrain_lenet\u001b[0;34m(characters)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder = 'results/part classifiers/'\n",
    "for number_of_character_classes in range(19, 36):\n",
    "    #declare folder variables for easier accessibility\n",
    "    current_folder = folder + str(number_of_character_classes) + '/'\n",
    "    better_folder = current_folder + 'better_ones/'\n",
    "    worse_folder = current_folder + 'worse_ones/'\n",
    "    previous_folder = folder + str(number_of_character_classes - 1) + '/'\n",
    "    previous_better_folder = previous_folder + 'better_ones/'\n",
    "    previous_worse_folder = previous_folder + 'worse_ones/'\n",
    "    \n",
    "    #good and bad characters obtained in the previous run\n",
    "    previous_better_characters = []\n",
    "    previous_worse_characters = []\n",
    "    \n",
    "    #previous good and bad characters\n",
    "    for file in os.listdir(previous_better_folder):\n",
    "        previous_better_characters.append(file.split(\"_\")[:number_of_character_classes-1])\n",
    "    for file in os.listdir(previous_worse_folder):\n",
    "        previous_worse_characters.append(file.split(\"_\")[:number_of_character_classes-1])\n",
    "    \n",
    "    already_done = list(map(lambda x: set(x[:-5].split('_')[:-1]), os.listdir(current_folder)))\n",
    "#     print(already_done)\n",
    "#     break\n",
    "    \n",
    "    print(previous_better_characters)    \n",
    "    print(previous_worse_characters) \n",
    "\n",
    "    #run a LeNet for every set\n",
    "    count = 0\n",
    "    for characters in previous_better_characters + previous_worse_characters:\n",
    "        for ch in data.keys():\n",
    "            if set(characters + [ch]) not in already_done:\n",
    "                if ch not in characters:\n",
    "                    results = train_lenet(characters + [ch])\n",
    "                    results.to_excel(current_folder + '_'.join(characters+[ch]) +\n",
    "                                     '_' + str(sum(results['true'] == 'True')/results.shape[0]) + '.xlsx')\n",
    "                    k.clear_session()\n",
    "                    count += 1\n",
    "                    print(count, number_of_character_classes)\n",
    "            else:\n",
    "              print(characters+[ch], \"already done!\")\n",
    "              count+=1\n",
    "        clear_output()\n",
    "    \n",
    "    #arranging good and bad excel files.\n",
    "    file_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for file in os.listdir(current_folder):\n",
    "        if file in ['better_ones', 'worse_ones']:\n",
    "            print(\"skipping folder\", file)\n",
    "            continue\n",
    "        file_list.append(file)\n",
    "        accuracy_list.append(float(file[:-5].split('_')[number_of_character_classes]))\n",
    "\n",
    "    #dataframe\n",
    "    file_to_accuracy_frame = pd.DataFrame({'file': file_list, 'accuracy': accuracy_list}).sort_values('accuracy')\n",
    "#     for display_dataframe(file_to_accuracy_frame.head(10))\n",
    "    for file in file_to_accuracy_frame['file'].head(10):\n",
    "        src = current_folder + file\n",
    "        dst = worse_folder + file\n",
    "        copyfile(src, dst)\n",
    "        \n",
    "    for file in file_to_accuracy_frame['file'].tail(10):\n",
    "        src = current_folder + file\n",
    "        dst = better_folder + file\n",
    "        copyfile(src, dst)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
